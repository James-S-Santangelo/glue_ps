{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac03c52-ca6d-4447-a260-2d06f54d69e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895e98b4-bc5e-4638-84dc-c9a47b07c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary with the major and minor allele at each SNP, inferred across all samples\n",
    "major_minor_dict = {chrom: {} for chrom in snakemake.params['chrom']}\n",
    "maf_prefix = snakemake.params[\"maf_prefix\"]\n",
    "\n",
    "for chrom in snakemake.params[\"chrom\"]:\n",
    "    glob_maf_path = f\"{maf_prefix}/allSamples/{chrom}/{chrom}_allSamples_snps.mafs.gz\"\n",
    "    glob_mafs = gzip.open(glob_maf_path,'rb').readlines()\n",
    "\n",
    "    for i, l in enumerate(glob_mafs):\n",
    "        if i != 0:\n",
    "            sl = l.strip().split(b\"\\t\")\n",
    "            pos = sl[1].decode(\"utf-8\")\n",
    "            REF = sl[2]\n",
    "            ALT = sl[3]\n",
    "            major_minor_dict[chrom][pos] = [REF, ALT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5984cd6b-8196-4ff1-bf76-b5029af5877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sites = 0\n",
    "for chrom, pos in major_minor_dict.items():\n",
    "    num_sites += len(pos)\n",
    "print(num_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e704e-51e4-4b50-b8e7-ccec18674c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary with the index of each SNP in each city and habitat\n",
    "def extract_positions(city, habitat):\n",
    "    pos_index_dict = {chrom: {} for chrom in snakemake.params['chrom']}\n",
    "    for chrom in snakemake.params[\"chrom\"]:\n",
    "        pos_path = f\"{maf_prefix}/byCity/{city}/{chrom}/{city}_{habitat}_{chrom}_snps.pos.gz\"\n",
    "        with gzip.open(pos_path,'rb') as pos:\n",
    "            lines = pos.readlines() \n",
    "            for i, l in enumerate(lines):\n",
    "                if i != 0:\n",
    "                    sl = l.strip().split(b\"\\t\")\n",
    "                    pos = sl[1].decode('utf-8')\n",
    "                    pos_index_dict[chrom][pos] = i\n",
    "    return pos_index_dict\n",
    "\n",
    "city_pos_index_dict = {city: {hab: [] for hab ibn snakemake.params[\"habitats\"]} for city in snakemake.params[\"cities\"]}\n",
    "for city in tqdm.tqdm(snakemake.params[\"cities\"]):\n",
    "    for hab in snakemake.params[\"habitats\"]:\n",
    "        city_pos_index_dict[city][hab] = extract_positions(city, hab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009c3486-bac3-4289-b415-35dc191b329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary with the sites missing from the per-city read count estimation\n",
    "# per-city analyses were run on filtered SNPs (i.e., post paralog removal)\n",
    "missing_dict = {city: {hab: {chrom: [] for chrom in snakemake.params[\"chrom\"]} for hab in snakemake.params[\"habitats\"]} for city in snakemake.params[\"cities\"]}\n",
    "\n",
    "def map_global_site_to_city_pos_indices(city, hab):\n",
    "    index_mapping_dict = {chrom: {} for chrom in snakemake.params['chrom']}\n",
    "    for chrom in snakemake.params[\"chrom\"]:\n",
    "        for g_pos in major_minor_dict[chrom].keys():\n",
    "            try:\n",
    "                pos_idx = city_pos_index_dict[city][hab][chrom][g_pos]\n",
    "                index_mapping_dict[chrom][g_pos] = pos_idx\n",
    "            except KeyError:\n",
    "                # print(f\"{chrom}: {gp} missing from {hab} habitat in {city}\")\n",
    "                missing_dict[city][hab][chrom].append(g_pos)\n",
    "    return index_mapping_dict\n",
    "\n",
    "city_index_mapping_dict = {city: {hab: [] for hab in snakemake.params[\"habitats\"]} for city in snakemake.params[\"cities\"]}\n",
    "for city in tqdm.tqdm(snakemake.params[\"cities\"]):\n",
    "    for hab in snakemake.params[\"habitats\"]:\n",
    "        city_index_mapping_dict[city][hab] = map_global_site_to_city_pos_indices(city, hab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0c5b3-a9e5-4509-afdb-e5eec1a25226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary with sites missing in at least one city\n",
    "combined_missing_site_dict = {chrom: set() for chrom in snakemake.params['chrom']}\n",
    "\n",
    "for city in tqdm.tqdm(snakemake.params[\"cities\"]):\n",
    "    for hab in snakemake.params[\"habitats\"]:\n",
    "        for chrom in snakemake.params[\"chrom\"]:\n",
    "            combined_missing_site_dict[chrom].update(missing_dict[city][hab][chrom])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809975a-628b-4c1e-93a0-a2ebd50e4b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sites = 0\n",
    "for chrom, pos in combined_missing_site_dict.items():\n",
    "    num_sites += len(pos)\n",
    "print(num_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa53f16-057b-4adf-bb1a-b3e4f09be5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to get read counts in each city at each SNPs that is present in all cities\n",
    "nucl_index_dict = {b'A': 0, b'C': 1, b'G': 2, b'T': 3}\n",
    "out_prefix = snakemake.params[\"out_prefix\"]\n",
    "\n",
    "def write_city_geno_file(city):\n",
    "    outpath = f\"{out_prefix}/{city}\"\n",
    "\n",
    "    if not os.path.exists(outpath):\n",
    "        os.makedirs(outpath)\n",
    "\n",
    "    outfile = f\"{outpath}/{city}.geno\"\n",
    "    with open(outfile, \"w\") as fout:\n",
    "        for chrom in snakemake.params[\"chrom\"]:\n",
    "            urb_counts_path = f\"{maf_prefix}/byCity/{city}/{chrom}/{city}_urban_{chrom}_snps.counts.gz\"\n",
    "            rur_counts_path = f\"{maf_prefix}/byCity/{city}/{chrom}/{city}_rural_{chrom}_snps.counts.gz\"\n",
    "            # urb_pos_path = maf_prefix + f\"byCity/{city}/{chrom}/{city}_{hab}_{chrom}_snps.pos.gz\"\n",
    "            # rur_pos_path = maf_prefix + f\"byCity/{city}/{chrom}/{city}_{hab}_{chrom}_snps.pos.gz\"\n",
    "            \n",
    "            urban_counts = gzip.open(urb_counts_path,'rb').readlines()\n",
    "            rural_counts = gzip.open(rur_counts_path,'rb').readlines()\n",
    "            # urban_pos = gzip.open(urb_pos_path,'rb').readlines()\n",
    "            # rural_pos = gzip.open(rur_pos_path,'rb').readlines()\n",
    "            \n",
    "            for g_pos in major_minor_dict[chrom].keys():\n",
    "                if g_pos in combined_missing_site_dict[chrom]:\n",
    "                    pass\n",
    "                else:\n",
    "                    try:\n",
    "                        urban_idx = city_pos_index_dict[city][\"urban\"][chrom].get(g_pos, None)\n",
    "                        rural_idx = city_pos_index_dict[city][\"rural\"][chrom].get(g_pos, None)\n",
    "                        \n",
    "                        # print(urban_counts[urban_idx].strip().split(b\"\\t\"), rural_counts[rural_idx].strip().split(b\"\\t\"))\n",
    "                        # print(REF, ALT)\n",
    "                        # assert g_pos == urban_pos[urban_idx].strip().split(b\"\\t\")[1]\n",
    "                        # assert g_pos == rural_pos[urban_idx].strip().split(b\"\\t\")[1]\n",
    "                        REF = major_minor_dict[chrom][g_pos][0]\n",
    "                        ALT = major_minor_dict[chrom][g_pos][1]\n",
    "                        \n",
    "                        urban_ref = urban_counts[urban_idx].strip().split(b\"\\t\")[nucl_index_dict[REF]].decode('utf-8')\n",
    "                        urban_alt = urban_counts[urban_idx].strip().split(b\"\\t\")[nucl_index_dict[ALT]].decode('utf-8')\n",
    "                        rural_ref = rural_counts[rural_idx].strip().split(b\"\\t\")[nucl_index_dict[REF]].decode('utf-8')\n",
    "                        rural_alt = rural_counts[rural_idx].strip().split(b\"\\t\")[nucl_index_dict[ALT]].decode('utf-8')\n",
    "                        # print(urban_ref, urban_alt, rural_ref, rural_alt)\n",
    "                        # print(\"========================\")\n",
    "                        fout.write(f\"{urban_ref} {urban_alt} {rural_ref} {rural_alt}\\n\")\n",
    "                    except IndexError:\n",
    "                        print(f\"{chrom}: {g_pos}\")\n",
    "                        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4756d16-0774-4f54-82e4-e514819e60be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write per-city read count files\n",
    "for city in tqdm.tqdm(snakemake.params[\"cities\"]):\n",
    "    write_city_geno_file(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36bc651-ecf3-473e-8ec8-b3ebc44d3224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write contrast file for c2 estimation\n",
    "for cont_file in snakemake.output[\"perCity_cont\"]:\n",
    "    with open(cont_file, \"w\") as fout:\n",
    "        fout.write(\"1 -1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68eb6d4-3756-440d-8488-20ad33cb58be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write poolsize files (i.e., number of alleles) for each city\n",
    "for city in tqdm.tqdm(snakemake.params[\"cities\"]):\n",
    "    with open(f\"{out_prefix}/{city}/{city}.poolsize\", \"w\") as fout:\n",
    "        for hab in snakemake.params[\"habitats\"]:\n",
    "            bam_list_file = [f for f in snakemake.input[\"perCity_bams\"] if city in f and hab in f][0]\n",
    "            with open(bam_list_file, \"r\") as fin:\n",
    "                lines = fin.readlines()\n",
    "                if hab == \"urban\":\n",
    "                    num_urban_alleles = len(lines) * 2\n",
    "                else:\n",
    "                    num_rural_alleles = len(lines) * 2\n",
    "        fout.write(f\"{num_urban_alleles} {num_rural_alleles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7064b49-2e21-4d5f-afd8-3033d7d35ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write order of sites\n",
    "with open(snakemake.output[\"site_order\"], \"w\") as fout:\n",
    "    for chrom in snakemake.params[\"chrom\"]:\n",
    "        for gp in major_minor_dict[chrom].keys():\n",
    "            if gp in combined_missing_site_dict[chrom]:\n",
    "                pass\n",
    "            else:\n",
    "                fout.write(f\"{chrom}\\t{gp}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e61bb2-14de-4626-a422-fd3c291084c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write file with missing sites for each city\n",
    "with open(snakemake.output[\"miss\"], \"w\") as fout:\n",
    "    for city in tqdm.tqdm(snakemake.params[\"cities\"]):\n",
    "        for hab in snakemake.params[\"habitats\"]:\n",
    "            for chrom in snakemake.params[\"chrom\"]:\n",
    "                for pos in missing_dict[city][hab][chrom]:\n",
    "                    # print(f\"{chrom}: {pos} missing from {hab} habitat in {city}\")\n",
    "                    fout.write(f\"{city}\\t{hab}\\t{chrom}\\t{pos}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
